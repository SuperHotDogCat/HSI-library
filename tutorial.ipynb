{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSI Library チュートリアル\n",
    "### 目次\n",
    "1. indian-pinesをダウンロード\n",
    "2. ライブラリをダウンロード\n",
    "3. ライブラリを利用したバンド削減\n",
    "4. DeepLearningモデルの訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. indian-pinesをダウンロード\n",
    "まずチュートリアルに使用するindian-pinesのデータをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存先を指定\n",
    "WORKDIR='./data'\n",
    "\n",
    "# データのダウンロード\n",
    "!wget https://www.ehu.eus/ccwintco/uploads/2/22/Indian_pines.mat -P $WORKDIR/indian_pines/ # data\n",
    "!wget https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat -P $WORKDIR/indian_pines/ # ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".matファイルでダウンロードされたデータを、このライブラリで扱うために.npyファイルへ変換します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def mat2npy(data_path):\n",
    "    print(data_path)\n",
    "    local, ext = os.path.splitext(data_path)\n",
    "    if ext != \".mat\":\n",
    "        print(f'{ext=}')\n",
    "        return\n",
    "    \n",
    "    data = loadmat(data_path)\n",
    "    print(data_path)\n",
    "    for key in data:\n",
    "        if isinstance(data[key], np.ndarray):\n",
    "            print(data[key].shape)\n",
    "            print(np.unique(data[key]))\n",
    "            save_path = os.path.join(\n",
    "                local,\n",
    "            )\n",
    "            np.save(save_path, data[key]) # npyに変換して保存\n",
    "\n",
    "# データのロード\n",
    "data_paths = [f'{WORKDIR}/indian_pines/Indian_pines.mat', f'{WORKDIR}/indian_pines/indian_pines_gt.mat']\n",
    "for data in data_paths:\n",
    "    mat2npy(data) \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ライブラリをダウンロード\n",
    "続いてライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryeを利用している場合\n",
    "!rye add hsi_feature_extraction --git https://github.com/SuperHotDogCat/HSI-library.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipの場合\n",
    "%pip install https://github.com/SuperHotDogCat/HSI-library.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ライブラリを利用したバンド削減\n",
    "実際にライブラリを使用して、HSIのバンド数を削減するアルゴリズムを実行します。\n",
    "ここでは、セレクションメソッドの一つである**VIF**を例に上げて説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのあるディレクトリ\n",
    "# WORKDIR='./data' \n",
    "\n",
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "# データのロード\n",
    "data_path = f'{WORKDIR}/indian_pines/Indian_pines.npy'\n",
    "label_path = f'{WORKDIR}/indian_pines/Indian_pines_gt.npy'\n",
    "\n",
    "# ライブラリでは（batch, channels, height, widt)の形でデータが渡されることを前提としているので、それに合わせてトランスポーズする。\n",
    "data = np.load(data_path)[np.newaxis,:32,:32,:].transpose([0,3,1,2])\n",
    "\n",
    "print(f'アルゴリズム適用前のシェイプ: {data.shape}')\n",
    "\n",
    "# ライブラリからVIFExtractorをインポート\n",
    "from hsi_feature_extraction.selection.vif import VIFExtractor\n",
    "processor = VIFExtractor()\n",
    "\n",
    "# VIFの学習\n",
    "processor.fit(data)\n",
    "\n",
    "# 学習したVIFの適用\n",
    "processed_data = processor(data)\n",
    "\n",
    "print(f'削減後のシェイプ: {processed_data.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 削減ライブラリを利用したDeepLearningモデルの訓練\n",
    "ライブラリを用いたDeeplerarningモデルの学習の例を示します。 \n",
    "基本は普通の場合と変わりませんが、学習の前にバンド削減アルゴリズムをfitし、学習時にはそれを適用する必要があります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import segmentation_models_pytorch as smp\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from hsi_feature_extraction.utils.utils import sampling_square_image\n",
    "from hsi_feature_extraction.utils.data import (\n",
    "    do_nothing,\n",
    "    do_nothing_sampling,\n",
    "    HSIDataWithSampling,\n",
    ")\n",
    "\n",
    "data_config = {\n",
    "        \"n_channels\": 220,\n",
    "        \"n_classes\": 17,\n",
    "    }\n",
    "\n",
    "# モデルとトレーニング、バリデーションの定義\n",
    "class LightningModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_channels: int,\n",
    "        n_classes: int,\n",
    "        model_name: str = \"unet\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = smp.Unet(\n",
    "            in_channels=n_channels, classes=n_classes\n",
    "        )\n",
    "        self.n_classes = n_classes\n",
    "        self.save_hyperparameters({\"model_name\": model_name})\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        x: (batch_size, channels, height, width)\n",
    "        label: (batch_size, height, width)\n",
    "        \"\"\"\n",
    "        x, label = batch\n",
    "        logits = self.model(x)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        self.log(\"training_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        x: (batch_size, channels, height, width)\n",
    "        label: (batch_size, height, width)\n",
    "        \"\"\"\n",
    "        x, label = batch\n",
    "        logits = self.model(x)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        self.log(\"validation_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(lr=1e-4, params=self.parameters())\n",
    "        return optimizer\n",
    "\n",
    "# dataクラスの作成 \n",
    "class LightningHSIData(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        label_path: Optional[str] = None,\n",
    "        sample_square_size: int = 64,\n",
    "        sampler=do_nothing_sampling,\n",
    "        transform=do_nothing,\n",
    "        split_ratio=0.75,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dataset = HSIDataWithSampling(\n",
    "            data_path=data_path,\n",
    "            label_path=label_path,\n",
    "            sample_square_size=sample_square_size,\n",
    "            sampler=sampler,\n",
    "            transform=transform,\n",
    "        )\n",
    "        self.split_ratio = split_ratio\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def prepare_data_per_node(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None):\n",
    "        dataset_size = len(self.dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        train_size = int(dataset_size * self.split_ratio)\n",
    "        valid_size = dataset_size - train_size\n",
    "\n",
    "        train_indices = indices[:train_size]\n",
    "        valid_indices = indices[train_size : train_size + valid_size]\n",
    "\n",
    "        self.train_dataset = Subset(self.dataset, train_indices)\n",
    "        self.valid_dataset = Subset(self.dataset, valid_indices)\n",
    "        del self.dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.valid_dataset)\n",
    "\n",
    "# シェイプ変換\n",
    "class ChannelTranspose(nn.Module):\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 3:\n",
    "            return x.transpose((2, 0, 1))\n",
    "        return x.transpose((0, 3, 1, 2))\n",
    "\n",
    "# ライブラリ適用\n",
    "class Pipeline:\n",
    "    def __init__(self, *args):\n",
    "        self.transforms = args\n",
    "    def __call__(self, x):\n",
    "        for transform in self.transforms:\n",
    "            x = transform(x)\n",
    "        return x\n",
    "\n",
    "# main\n",
    "trainer = pl.Trainer(devices=1, max_epochs=3, enable_checkpointing=False)\n",
    "\n",
    "\n",
    "processor = VIFExtractor()\n",
    "data = np.load(data_path)[np.newaxis,:32,:32,:].transpose([0,3,1,2])\n",
    "print(data.shape)\n",
    "processor.fit(data) #バンド削減アルゴリズムの学習\n",
    "del data\n",
    "\n",
    "data_loader = LightningHSIData(\n",
    "    data_path=data_path,\n",
    "    label_path=label_path,\n",
    "    sample_square_size=64,\n",
    "    sampler=sampling_square_image,\n",
    "    transform=Pipeline(\n",
    "        ChannelTranspose(),\n",
    "        processor #　バンド削減の適用\n",
    "    ),\n",
    ")\n",
    "data_loader.setup()\n",
    "model = LightningModel(processor.get_num_channels(), data_config[\"n_classes\"])\n",
    "trainer.fit(\n",
    "    model,\n",
    "    data_loader.train_dataloader(),\n",
    "    data_loader.val_dataloader(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
