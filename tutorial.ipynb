{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HSI Library チュートリアル\n",
    "### 目次\n",
    "1. indian-pinesをダウンロード\n",
    "2. ライブラリをダウンロード\n",
    "3. ライブラリを利用したバンド削減\n",
    "4. DeepLearningモデルの訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. indian-pinesをダウンロード\n",
    "まずチュートリアルに使用するindian-pinesのデータをダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/masaya/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-09-23 14:16:37--  https://www.ehu.eus/ccwintco/uploads/2/22/Indian_pines.mat\n",
      "www.ehu.eus (www.ehu.eus) をDNSに問いあわせています... 158.227.0.65, 2001:720:1410::65\n",
      "www.ehu.eus (www.ehu.eus)|158.227.0.65|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 6296374 (6.0M)\n",
      "‘./data/indian_pines/Indian_pines.mat.1’ に保存中\n",
      "\n",
      "Indian_pines.mat.1    0%[                    ]       0  --.-KB/s               ^C\n",
      "Will not apply HSTS. The HSTS database must be a regular and non-world-writable file.\n",
      "ERROR: could not open HSTS store at '/home/masaya/.wget-hsts'. HSTS will be disabled.\n",
      "--2024-09-23 14:16:39--  https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat\n",
      "www.ehu.eus (www.ehu.eus) をDNSに問いあわせています... 158.227.0.65, 2001:720:1410::65\n",
      "www.ehu.eus (www.ehu.eus)|158.227.0.65|:443 に接続しています... 接続しました。\n",
      "HTTP による接続要求を送信しました、応答を待っています... 200 OK\n",
      "長さ: 1125 (1.1K)\n",
      "‘./data/indian_pines/Indian_pines_gt.mat.1’ に保存中\n",
      "\n",
      "Indian_pines_gt.mat 100%[===================>]   1.10K  --.-KB/s    in 0s      \n",
      "\n",
      "2024-09-23 14:16:40 (289 MB/s) - ‘./data/indian_pines/Indian_pines_gt.mat.1’ へ保存完了 [1125/1125]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 保存先を指定\n",
    "WORKDIR='./data'\n",
    "\n",
    "# データのダウンロード\n",
    "!wget https://www.ehu.eus/ccwintco/uploads/2/22/Indian_pines.mat -P $WORKDIR/indian_pines/ # data\n",
    "!wget https://www.ehu.eus/ccwintco/uploads/c/c4/Indian_pines_gt.mat -P $WORKDIR/indian_pines/ # ground truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".matファイルでダウンロードされたデータを、pythonで扱いやすい.npyファイルへ変換します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/indian_pines/Indian_pines_gt.mat\n",
      "./data/indian_pines/Indian_pines_gt.mat\n",
      "(145, 145)\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16]\n",
      "./data/indian_pines/Indian_pines.mat\n",
      "./data/indian_pines/Indian_pines.mat\n",
      "(145, 145, 220)\n",
      "[ 955  961  962 ... 9279 9360 9604]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "\n",
    "def mat2npy(data_path):\n",
    "    print(data_path)\n",
    "    local, ext = os.path.splitext(data_path)\n",
    "    if ext != \".mat\":\n",
    "        print(f'{ext=}')\n",
    "        return\n",
    "    \n",
    "    data = loadmat(data_path)\n",
    "    print(data_path)\n",
    "    for key in data:\n",
    "        if isinstance(data[key], np.ndarray):\n",
    "            print(data[key].shape)\n",
    "            print(np.unique(data[key]))\n",
    "            save_path = os.path.join(\n",
    "                local,\n",
    "            )\n",
    "            np.save(save_path, data[key]) # npyに変換して保存\n",
    "\n",
    "# データのロード\n",
    "data_paths = [f'{WORKDIR}/indian_pines/Indian_pines.mat', f'{WORKDIR}/indian_pines/indian_pines_gt.mat']\n",
    "for data in data_paths:\n",
    "    mat2npy(data) \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/masaya/in-spector-project/HSI-library\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.ライブラリをダウンロード\n",
    "続いてライブラリをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ryeを利用している場合\n",
    "!rye add hsi_feature_extraction --git https://github.com/SuperHotDogCat/HSI-library.git "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipの場合\n",
    "%pip install https://github.com/SuperHotDogCat/HSI-library.git "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. ライブラリを利用したバンド削減\n",
    "実際にライブラリを使用して、HSIのバンド数を削減するアルゴリズムを実行します。\n",
    "ここでは、セレクションメソッドの一つである**VIF**を例に上げて説明します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 32, 32, 220)\n"
     ]
    }
   ],
   "source": [
    "# データのロード\n",
    "data_path = f'{WORKDIR}/indian_pines/Indian_pines.npy'\n",
    "label_path = f'{WORKDIR}/indian_pines/Indian_pines_gt.npy'\n",
    "data = np.load(data_path)[np.newaxis,:32,:32,:]\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class VIFExtractor with abstract method get_num_channels",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ライブラリからVIFExtractorをインポート\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhsi_feature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mselection\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvif\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VIFExtractor\n\u001b[0;32m----> 3\u001b[0m processor \u001b[38;5;241m=\u001b[39m \u001b[43mVIFExtractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# アルゴリズムの適用\u001b[39;00m\n\u001b[1;32m      6\u001b[0m processor\u001b[38;5;241m.\u001b[39mfit(data)\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class VIFExtractor with abstract method get_num_channels"
     ]
    }
   ],
   "source": [
    "# ライブラリからVIFExtractorをインポート\n",
    "from hsi_feature_extraction.selection.vif import VIFExtractor\n",
    "processor = VIFExtractor()\n",
    "\n",
    "# アルゴリズムの適用\n",
    "processor.fit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
